

"/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car2.txt","/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car3.txt","/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car4.txt","/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car5.txt","/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car6.txt"

import sys.process._

/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car2.txt
/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car3.txt
/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car4.txt
/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car5.txt
/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car6.txt

val listofFilesInFolder="hdfs dfs -ls /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/"!!

val filesToRead=listofFilesInFolder.split("\n").slice(1,listofFilesInFolder.split("\n").size).map(x => (x.contains("Extra") || x.contains("_diff_")) match {case true => println("") case _ => x.substring(x.indexOf("/"),x.size)} ).filter(_ .toString !="()").map(_.asInstanceOf[String])//.map(_.toString)

val commonCols=Seq("Vehicle_id","model","brand","year","month","miles","intake_date_time")

var dfTest:org.apache.spark.sql.DataFrame=null
for(file <- filesToRead)
dfTest match {
case null => 
dfTest=spark.read.option("inferSchema","true").option("header","true").option("delimiter","|").csv(file).selectExpr(commonCols:_*)
case _ =>dfTest=dfTest.union(spark.read/*.format("com.databricks.spark.csv")*/.option("inferSchema","true").option("header","true").option("delimiter","|").csv(file).selectExpr(commonCols:_*))
}


df.repartition(col("month")).rdd.partitions.size // default partition size will be 200
res10: Int = 200

df.repartition(100).rdd.partitions.size // default partition size is 100, we can specify the number partitions 


df.repartition(5,col("month")).rdd.partitions.size   // default partition size will be 200, we can override it with number and then it'll put all the same column values and put it in the  same partition 

df.repartition(df.select("month").distinct.count.toInt,col("month")).rdd.partitions.size




scala> dfTest.repartition(col("brand"),col("model")).rdd.partitions.size
res13: Int = 200

scala> dfTest.repartition(col("brand")).rdd.partitions.size
res14: Int = 200

scala> dfTest.repartition(3).rdd.partitions.size
res15: Int = 3

scala> dfTest.repartition(3,col("brand")).rdd.partitions.size
res16: Int = 3

scala> dfTest.repartition(5,col("brand")).rdd.partitions.size
res17: Int = 5

scala> dfTest.repartition(col("brand"),col("model"),col("miles")).rdd.partitions.size
res19: Int = 200

Seq(col("brand"),col("model"),col("miles"))
res20: Seq[org.apache.spark.sql.Column] = List(brand, model, miles)

dfTest.repartition(res20:_*).rdd.partitions.size
res21: Int = 200

