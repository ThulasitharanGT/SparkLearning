creating table:
 create  'cardata','software','hardware','other'

inserting data to table:

put 'cardata','v001_H','hardware:alloy_wheels','yes'
put 'cardata','v001_H','hardware:anti_Lock_break','yes'
put 'cardata','v001_H','software:electronic_breakforce_distribution','yes'
put 'cardata','v001_H','software:terrain_mode','yes'
put 'cardata','v001_H','software:traction_control','yes'
put 'cardata','v001_H','software:stability_control','yes'
put 'cardata','v001_H','software:cruize_control','yes'
put 'cardata','v001_H','other:make','hyundai'
put 'cardata','v001_H','other:model','i10'
put 'cardata','v001_H','other:variant','sportz'

// 2 extra columns

put 'cardata','v002_H','hardware:alloy_wheels','yes'
put 'cardata','v002_H','hardware:anti_Lock_break','yes'
put 'cardata','v002_H','software:electronic_breakforce_distribution','yes'
put 'cardata','v002_H','software:rev_indicator','yes'
put 'cardata','v002_H','software:terrain_mode','yes'
put 'cardata','v002_H','software:traction_control','yes'
put 'cardata','v002_H','software:stability_control','yes'
put 'cardata','v002_H','software:cruize_control','yes'
put 'cardata','v002_H','software:drift_control','yes'
put 'cardata','v002_H','other:make','hyundai'
put 'cardata','v002_H','other:model','i20'
put 'cardata','v002_H','other:variant','sportz'

// 1 less column 
inserting data to table:


put 'cardata','v003_H','hardware:anti_Lock_break','yes'
put 'cardata','v003_H','software:electronic_breakforce_distribution','yes'
put 'cardata','v003_H','software:terrain_mode','yes'
put 'cardata','v003_H','software:traction_control','yes'
put 'cardata','v003_H','software:stability_control','yes'
put 'cardata','v003_H','software:cruize_control','yes'
put 'cardata','v003_H','other:make','hyundai'
put 'cardata','v003_H','other:model','santro'
put 'cardata','v003_H','other:variant','sportz'

scan 'cardata'

// hive table

create external table car_detail_hbase(
car_id string,
alloy_wheels string,
anti_Lock_break string,
electronic_breakforce_distribution string,
terrain_mode string,
traction_control string,
stability_control string,
cruize_control string,
make string,
model string,
variant string)
STORED BY
'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES
("hbase.columns.mapping"=":key,hardware:alloy_wheels,hardware:anti_Lock_break,
software:electronic_breakforce_distribution,software:terrain_mode,
software:traction_control,software:stability_control,software:cruize_control,other:make,
other:model,other:variant")
TBLPROPERTIES
("hbase.table.name"="cardata")


-> wat ever mapped is bought. extra is dropped, cols not present for the particular row, null is returned

//

hive

create external table cardata_silver
(Vehicle_id string ,
miles int ,
intake_date_time timestamp ,
number_of_owners int)
PARTITIONED BY
(
brand string ,
model string,
year int ,
month int 
)
STORED AS PARQUET
LOCATION
'/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTableAddingColumnInBetween_Silver/';


brand=Renault/model=Megane/year=2010/month


/home/raptor/Softwares/hbase-1.2.5
/home/raptor/Softwares/hadoop-2.7.3

Jackson version issue. 3.2.10 is the correct one


spark-shell --packages com.hortonworks:shc:1.1.1-2.1-s_2.11,com.hortonworks:shc-core:1.1.1-2.1-s_2.11   --repositories https://repository.apache.org/content/repositories/releases  --files /home/raptor/Softwares/hbase-1.2.5/conf/hbase-site.xml


import org.apache.spark.sql.execution.datasources.hbase._
import spark.implicits._
import org.apache.hadoop.hbase.client.{HBaseAdmin, Result}
import org.apache.hadoop.hbase.{ HBaseConfiguration, HTableDescriptor }
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.io.ImmutableBytesWritable

def carCatalog = s"""{
"table":{"namespace":"default", "name":"cardata"},
"rowkey":"key",
"columns":{
"vehicle_id":{"cf":"rowkey", "col":"key", "type":"string"},
"alloy_wheels":{"cf":"hardware", "col":"alloy_wheels", "type":"string"},
"anti_Lock_break":{"cf":"hardware", "col":"anti_Lock_break", "type":"string"},
"electronic_breakforce_distribution":{"cf":"software", "col":"electronic_breakforce_distribution", "type":"string"},
"terrain_mode":{"cf":"software", "col":"terrain_mode", "type":"string"},
"traction_control":{"cf":"software", "col":"traction_control", "type":"string"},
"stability_control":{"cf":"software", "col":"stability_control", "type":"string"},
"cruize_control":{"cf":"software", "col":"cruize_control", "type":"string"},
"make":{"cf":"other", "col":"make", "type":"string"},
"model":{"cf":"other", "col":"model", "type":"string"},
"variant":{"cf":"other", "col":"variant", "type":"string"}
}
}""".stripMargin


spark.conf.set("spark.hbase.host", "localhost")

val hbaseDF=spark.read.options(Map(HBaseTableCatalog.tableCatalog->carCatalog)).format("org.apache.spark.sql.execution.datasources.hbase").load()

case class carRecord(
    vehicle_id: String,
    alloy_wheels: String,
    anti_Lock_break: String,
    electronic_breakforce_distribution: String,
    terrain_mode: String,
    traction_control: String,
    stability_control: String,
    cruize_control: String,
    make: String,
    model: String,
    variant: String
    )

val newCarDF = spark.createDataFrame(Seq(("v004_H", "yes", "yes", "yes","yes","yes","yes","yes","Suzuki","Baleno","Delta"))).toDF("vehicle_id","alloy_wheels","anti_Lock_break","electronic_breakforce_distribution","terrain_mode","traction_control","stability_control","cruize_control","make","model","variant")

var carData = new Array[carRecord](1)
carData(0) = newCarRecord

val sc=spark.sparkContext
newCarDF.write.options(Map(HBaseTableCatalog.tableCatalog -> carCatalog, HBaseTableCatalog.newTable -> "5")).format("org.apache.spark.sql.execution.datasources.hbase").save()


org.json4s:json4s-jackson_2.11:3.3.0,org.json4s:json4s-scalap_2.11:3.3.0,org.json4s:json4s-core_2.11:3.3.0,org.json4s:json4s-ast_2.11:3.3.0


zookeeper start:

sh /home/raptor/Softwares/kafka_2.12-2.3.1/bin/zookeeper-server-start.sh  /home/raptor/Softwares/kafka_2.12-2.3.1/config/zookeeper.properties

sh /home/raptor/Softwares/kafka_2.12-2.3.1/bin/zookeeper-server-stop.sh  

hbase start stop:

sh /home/raptor/Softwares/hbase-1.2.5/bin/start-hbase.sh

/home/raptor/Softwares/hbase-1.2.5/bin/stop-hbase.sh

hadoop start:

/home/raptor/Softwares/hadoop-2.7.3/sbin/start-all.sh

/home/raptor/Softwares/hadoop-2.7.3/sbin/stop-all.sh





