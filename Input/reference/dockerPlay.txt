docker run -t -i ubuntu:latest

apt-get update

apt-get install -y openjdk-8-jdk

apt-get install python

apt-get install python-pip

//apt-get install -y python3-pip python3-dev              ->> converting python3 to python shell 

//pip3 install --upgrade pip

//cd /usr/local/bin

// ln -s /usr/bin/python3 python   

  

 

pip install pyspark

apt-get install git

apt-get install wget

mkdir temp

cd temp

wget "http://us.mirrors.quenda.co/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz"

//else wget "http://mirrors.ocf.berkeley.edu/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz"

apt-get install nano

tar -xzvf spark-2.4.4-bin-hadoop2.7.tgz

SPARK_HOME=/usr/local/bin/temp/spark-2.4.4-bin-hadoop2.7

path=path:$SPARK_HOME/bin

 

or nano ~/.bashrc

 

cd temp

wget "https://introcs.cs.princeton.edu/java/data/sdss1738478.csv"

wget "https://introcs.cs.princeton.edu/java/data/sdss6949386.csv"

 

 

df = spark.read.option("header", "true").csv("/temp/sdss6949386.csv")

df.show(n=2)

df.show(5)

 

from pyspark.sql.functions import lit

from pyspark.sql.functions import col

from pyspark.sql.functions import *

from pyspark.sql.types import *

from pyspark.sql.DataFrame import *  --> notworking

from pyspark.sql import functions

 

dfnew=df.withColumn("name",lit("cool"))

dfnew.show(5,truncate=False)

dfnew.show(5,truncate=False, vertical=True)

/temp

 

import os

os.system("command")

 

 

/>>>>>> schema mannualy

df = spark.read.option("header", "false").schema("objectid long,right_ascension float,declination float,ultraviolet double,green double,red double,infrared  double,z string").csv("/temp/sdss6949386.csv").withColumn("Index",monotonically_increasing_id()).filter("Index > 0").drop("Index")

 

df.selectExpr("min(infrared)","max(infrared)").show(1)

+-------------+----------------+                                               

|min(infrared)|   max(infrared)|

+-------------+----------------+

|      -9999.0|34.4253463745117|

+-------------+----------------+

 

df.select("infrared").distinct().show(10)

 

// with Column and when

newDF=df.withColumn("name",when(col("infrared") < 0.0 , "RAM").otherwise("Hellcat"))

 

newDF=df.withColumn("name",when(col("infrared") < 0.0 , "RAM").when((col("infrared") > 0.0) & (col("infrared") < 2.0) , "Charger").when((col("infrared") >2.0) & (col("infrared") < 4.0) , "Challenger").when((col("infrared") >4.0) & (col("infrared") < 6.0) , "Deamon").when((col("infrared") >6.0) & (col("infrared") < 8.0) , "Dart").when((col("infrared") >10.0) & (col("infrared") < 12.0) , "RedEye").when((col("infrared") >14.0) & (col("infrared") < 16.0) , "Viper").when((col("infrared") >22.0) & (col("infrared") < 24.0) , "Classic").when((col("infrared") >32.0) & (col("infrared") < 35.0) , "Venom").otherwise("Hellcat"))

 

newDF.groupBy("name").agg(max("ultraviolet")).show(10)

 

newDF.groupBy("name").agg(sum("ultraviolet")).show(10)

 

newDF.groupBy("name").agg(abs(sum("ultraviolet"))).show(10)

 

newDF.groupBy("name").agg(countDistinct("ultraviolet")).show(10)

 

 

/>>>>>> header mannualy

dfWithoutHeader = spark.read.option("header", "false").option("inferSchema", "false").csv("/temp/sdss6949386.csv").withColumn("Index",monotonically_increasing_id()).filter("Index > 0").drop("Index").toDF("objectid","right_ascension","declination","ultraviolet","green","red","infrared ","z")

 

// dynamically passing

 

dfWithoutHeader = spark.read.option("header", "false").option("inferSchema", "false").csv("/temp/sdss6949386.csv").withColumn("Index",monotonically_increasing_id()).filter("Index > 0").drop("Index")

headerString="objectid,right_ascension,declination,ultraviolet,green,red,infrared,z"

headerArray=headerString.split(",")

dfWithHeader=dfWithoutHeader.toDF(*headerArray)

 

 

 

dfTemp = spark.read.option("header", "true").option("inferSchema", "false").csv("/temp/sdss6949386.csv")

 

 

 

newDF2=newDF

 

newDF3=newDF2.withColumn("Index",monotonically_increasing_id()+1)

 

carMetaDataDF=spark.createDataFrame([("RAM","Dodge","Truck"),("Dart","Dodge","Sedan"),("Deamon","Dodge","Drag"),("Hellcat","Dodge","Drag"),("RedEye","Dodge","Drag"),("Venom","Hennesey","Race"),("Viper","Dodge","Race"),("Classic","GM","Commuter")]).toDF("name","make","type")

 

joinDf=newDF.join(carMetaDataDF,col("name")==col("name"),"inner") //diff coloumn names. Ambiguous

 

joinDf=newDF.join(carMetaDataDF,["name"],"inner")

 

joinDfLeft=newDF.join(carMetaDataDF,["name"],"inner")

 

joinDf.write.mode("overwrite").option("header","true").option("delimiter","|").partitionBy("type","make","name").csv("/temp/Output/DataOne")

 

tempString=os.system("ls /temp/Output/DataOne/type=Drag/make=Dodge")

print(tempString)

--> 0

// works in python 3.7

import subprocess

result = subprocess.run(['ls','/temp/Output/DataOne/type=Drag/make=Dodge'], stdout=subprocess.PIPE,stderr=subprocess.STDOUT)

tempString=result.stdout.decode("utf-8")

stringFinal=tempString.split("\n")

 

for

 

 

b"abcde".decode("utf-8")

 

 

newDF.select("name").distinct().show(100,truncate=False)

 

 

 

 

 

joinDf=newDF.join(newDF3,["objectid"],"inner")

 

 

// won tWork

dfDirect=spark.read.option("header", "false").option("inferSchema", "false").csv("https://introcs.cs.princeton.edu/java/data/sdss6949386.csv")

 

 

 

def comparingDfSchema(df1,df2) :

                if (df1.union(df2))

                                return true

                elif

                                return false

 

   
