
export SPARK_MAJOR_VERSION=2​
spark-shell​
​
​
# Partition Information varchar(16777216),# col_name​
​
case class ddlFormatterClass(tableName:String,rawDDL:String,currDDL:String,histDDL:String)​
case class ddlFormatterClassAsSingleColumn(tableDDL:String)​
​
import org.apache.spark.sql.Row​
import scala.collection.mutable.ListBuffer​
import scala.collection.mutable.ArrayBuffer​
import org.apache.spark.sql.DataFrame​
​
val varcharDataType="varchar(16777216)"​
val snowflakeSchemaAndDb="NBCU_DATA_INTEG_META_PROD.TIME_MACHINE."​
val RawDdlString:String="CREATE OR REPLACE TABLE "+snowflakeSchemaAndDb+"RAW_"​
val CurrDdlString:String="CREATE OR REPLACE TABLE "+snowflakeSchemaAndDb+"CURR_"​
//val CurrDdlString:String="CREATE OR REPLACE TABLE "+snowflakeSchemaAndDb+""​
val HistDdlString:String="CREATE OR REPLACE TABLE "+snowflakeSchemaAndDb+"HIST_"​
val tableList=spark.sql("show tables in time_machine_prod")​
val tableNameList=tableList.select("tableName").collect.map(x => x.toString.replace("[","").replace("]","").trim)​
val DDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
val DDFListBuffer:ListBuffer[ddlFormatterClass]= new ListBuffer[ddlFormatterClass]()​
val srcTableList=Array("ADVERTISER","ADVERTISER_GRP_CURR_SMT","ADVERTISER_GRP_XREF_CURR_SMT","AGENCY","AGENCY_GRP_CURR_SMT","AGENCY_GRP_XREF_CURR_SMT","CDW_CALENDAR_DATE","CDW_CALENDAR_MONTH","CDW_CALENDAR_QTR","CDW_CALENDAR_SEASON_YEAR","CDW_CALENDAR_TYPE","CDW_CALENDAR_WEEK","CDW_CALENDAR_YEAR","DAYPART","DAYPART_GRP_MSTR","DAYPART_GRP_XREF_MSTR","DEAL_MARKET","DEAL_STATUS","DEAL_STATUS_GRP","DEAL_STATUS_GRP_XREF","DEAL_TYPE","DEAL_TYPE_GRP","DEAL_TYPE_GRP_XREF","DEAL_USR","DEMO","DEMO_GRP","DEMO_GRP_XREF","EPISODE","EVENT_TYPE","INDUSTRY_CATEGORY","INDUSTRY_CATEGORY_GRP_SMT","INDUSTRY_CATEGORY_GRP_XREF_SMT","INDUSTRY_SUB_CATEGORY","INDUSTRY_SUB_SUB_CATEGORY","KEY_PROPERTY_DAYPART","PARENT_AGENCY_GRP_CURR_SMT","PARENT_AGY_GRP_XREF_CURR_SMT","PROGRAM","PROPERTY","PROPERTY_DIVISION","PROPERTY_GRP","PROPERTY_GRP_XREF","RATECLASS","SALES_PLAN_SUB_STATUS","SELLING_TITLE","SELLING_TITLE_CAT_GRP","SELLING_TITLE_CAT_GRP_XREF","SELLING_TITLE_GENRE","SELLING_TITLE_GRP","SELLING_TITLE_GRP_TRANS","SELLING_TITLE_GRP_XREF","SELLING_TITLE_GRP_XREF_TRANS","SELLING_TITLE_GRP_XRF_TRS_MSTR","SELLING_TITLE_SET_GRP","SELLING_TITLE_SET_GRP_XREF","SOURCE_SYSTEM","TRAFFIC_LOCK_STATUS","TRAFFIC_STATUS","TRAFFIC_UNIT_STATUS","UNIT_TYPE","V_CDW_CALENDAR_DATE_FISC","V_CDW_CALENDAR_MONTH_FISC","V_CDW_CALENDAR_QTR_FISC","CALENDAR_DATE","CALENDAR_BROADCAST_QUARTER","DEAL_MARKET_GRP_XREF","DEAL_MARKET_GRP","COMMERCIAL_SUB_TYPE","INVENTORY_UNIT_TYPE","INVENTORY_UNIT_TYPE_GRP_XREF","INVENTORY_UNIT_TYPE_GRP","BUSINESS_TYPE","RH_PROPERTY_GRP","RH_BUSINESS_GROUP","RH_BUSINESS_PROPERTY_GRP","RH_LINE_OF_BUSINESS","RH_SELLING_VERTICAL","RH_SUB_DIVISION","RH_DIVISION","sales_detail_fact","sales_option_qtr_fact","smo_week_inventory","traf_inventory_fact").map(x => x.toUpperCase) // weekly tables are as a file, load it in a df and do a collect (This approach works only in 68 due to access denied in 67)​
​
​
// val srcTableList=spark.read.option("header","false").option("delimiter","~").csv("/home/206478585/ddlFilesTimeMachine/timeMachineWeeklyTablelist.txt").select("_c0").collect.map(x => x.toString.substring(1,x.toString.length -1).trim)​
​
// all yield the same result.​
//.map(x => x.toString.substring(1,x.toString.length -1).trim) ​
//.map(x => x.toString.substring(x.toString.indexOf("[")+1,x.toString.lastIndexOf("]")).trim) ​
//.map(x => x.toString.replace("[","").replace("]","").trim) // works perfectly in 68​
​
//.substring(1, (x.length) -1)​
​
​
val describeOutput=spark.sql(s"describe table time_machine_prod.sales_option_qtr_fact_orc")​
val colNames=describeOutput.select("col_name").collect.map(x => x.toString.replace("[","").replace("]","").trim)​
val listNumberOf=colNames.indexOf("# Partition Information")​
val colNamesFinal=colNames.slice(0,listNumberOf)​
​
​
var finalTableList=ArrayBuffer[String]()​
for (tableName <- tableNameList) // taking ddl for relavant table ​
if(  srcTableList contains tableName.dropRight(4).toUpperCase )​
finalTableList+=tableName​
​
val srcTableListORC=srcTableList.map(x => x+"_orc")​
def ddlCreater(DDL : DataFrame,fileAndPAth:String) =DDL.rdd.saveAsTextFile("/home/206478585/ddlTemp/"+fileAndPAth+".table.ddl")​
​
​
var finalTableListFinal=ArrayBuffer[String]()​
for (tableName <- srcTableListORC)​
{​
val describeOutput=spark.sql(s"describe table time_machine_prod.${tableName}").withColumn("rowNum",monotonically_increasing_id)​
//describeOutput.show​
val colNames=describeOutput.select("col_name").collect.map(x => x.toString.replace("[","").replace("]","").trim)​
val listNumberOf=colNames.indexOf("# Partition Information")​ //ignoring partition info
var colNamesFinal:Array[String]=null​
listNumberOf match {case x if (x== -1) =>colNamesFinal=colNames ;case x if (x>0) =>colNamesFinal=colNames.slice(0,listNumberOf) ; case _ => colNamesFinal=colNames}​
var RawDdlStringTemp=RawDdlString+tableName.dropRight(4).toUpperCase+"("​
val RawTableNameTemp=snowflakeSchemaAndDb+"RAW_"+tableName.dropRight(4).toUpperCase​
//var CurrDdlStringTemp=CurrDdlString+tableName.dropRight(4).toUpperCase+"_ORC("​
var CurrDdlStringTemp=CurrDdlString+"CURR_"+tableName.dropRight(4).toUpperCase+"("​
val CurrTableNameTemp=snowflakeSchemaAndDb+"CURR_"+tableName.dropRight(4).toUpperCase​
var HistDdlStringTemp=HistDdlString+tableName.dropRight(4).toUpperCase+"("​
val HistTableNameTemp=snowflakeSchemaAndDb+"HIST_"+tableName.dropRight(4).toUpperCase​
val numOfCols=colNamesFinal.size​
var colCount:Int=0​
for (colName <- colNamesFinal)​
{​
colCount=colCount+1​
if (colCount==1)​
{​
RawDdlStringTemp=RawDdlStringTemp+colName+" "+varcharDataType​
CurrDdlStringTemp=CurrDdlStringTemp+colName+" "+varcharDataType​
HistDdlStringTemp=HistDdlStringTemp+colName+" "+varcharDataType​
}​
else if (colCount<numOfCols)​
{​
RawDdlStringTemp=RawDdlStringTemp+","+colName+" "+varcharDataType​
CurrDdlStringTemp=CurrDdlStringTemp+","+colName+" "+varcharDataType​
HistDdlStringTemp=HistDdlStringTemp+","+colName+" "+varcharDataType​
}​
else​
{​
RawDdlStringTemp=RawDdlStringTemp+");"​
CurrDdlStringTemp=CurrDdlStringTemp+");"​
HistDdlStringTemp=HistDdlStringTemp+",ETLLOGID VARCHAR(16777216),SOURCEID VARCHAR(16777216),FILEID VARCHAR(16777216),FILEDT VARCHAR(16777216));"​
//println(RawDdlStringTemp) ​
println(CurrDdlStringTemp)​
//println(HistDdlStringTemp)​
}​
}​
}​
​
​
DDFListBuffer+=ddlFormatterClass(tableName.dropRight(4),RawDdlStringTemp,CurrDdlStringTemp,HistDdlStringTemp) // DDL generation​
val rawDDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
rawDDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(RawDdlStringTemp)​
//ddlCreater(DDFListBuffer.toSeq.toDF,RawTableNameTemp)​
val currDDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
currDDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(CurrDdlStringTemp)​
//ddlCreater(DDFListBuffer.toSeq.toDF,CurrTableNameTemp)​
val histDDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
histDDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(HistDdlStringTemp)​
//ddlCreater(DDFListBuffer.toSeq.toDF,HistTableNameTemp) ​
DDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(RawDdlStringTemp)​
DDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(CurrDdlStringTemp)​
DDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(HistDdlStringTemp)