-------------------------------------​
​
export SPARK_MAJOR_VERSION=2​
spark-shell​
​
​
# Partition Information varchar(16777216),# col_name​
​
case class ddlFormatterClass(tableName:String,rawDDL:String,currDDL:String,histDDL:String)​
case class ddlFormatterClassAsSingleColumn(tableDDL:String)​
​
import org.apache.spark.sql.Row​
import scala.collection.mutable.ListBuffer​
val varcharDataType="varchar(16777216)"​
val snowflakeSchemaAndDb="NBCU_DATA_INTEG_META_PROD.TIME_MACHINE."​
val RawDdlString:String="CREATE OR REPLACE TABLE "+snowflakeSchemaAndDb+"RAW_"​
val CurrDdlString:String="CREATE OR REPLACE TABLE "+snowflakeSchemaAndDb+"CURR_"​
val HistDdlString:String="CREATE OR REPLACE TABLE "+snowflakeSchemaAndDb+"HIST_"​
val tableList=spark.sql("show tables in time_machine_prod")​
val tableNameList=tableList.select("tableName").collect.map(x => x.toString.replace("[","").replace("]","").trim)​
val DDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
val DDFListBuffer:ListBuffer[ddlFormatterClass]= new ListBuffer[ddlFormatterClass]()​
val srcTableList=Array("ADVERTISER","ADVERTISER_GRP_CURR_SMT","ADVERTISER_GRP_XREF_CURR_SMT","AGENCY","AGENCY_GRP_CURR_SMT","AGENCY_GRP_XREF_CURR_SMT","CDW_CALENDAR_DATE","CDW_CALENDAR_MONTH","CDW_CALENDAR_QTR","CDW_CALENDAR_SEASON_YEAR","CDW_CALENDAR_TYPE","CDW_CALENDAR_WEEK","CDW_CALENDAR_YEAR","DAYPART","DAYPART_GRP_MSTR","DAYPART_GRP_XREF_MSTR","DEAL_MARKET","DEAL_STATUS","DEAL_STATUS_GRP","DEAL_STATUS_GRP_XREF","DEAL_TYPE","DEAL_TYPE_GRP","DEAL_TYPE_GRP_XREF","DEAL_USR","DEMO","DEMO_GRP","DEMO_GRP_XREF","EPISODE","EVENT_TYPE","INDUSTRY_CATEGORY","INDUSTRY_CATEGORY_GRP_SMT","INDUSTRY_CATEGORY_GRP_XREF_SMT","INDUSTRY_SUB_CATEGORY","INDUSTRY_SUB_SUB_CATEGORY","KEY_PROPERTY_DAYPART","PARENT_AGENCY_GRP_CURR_SMT","PARENT_AGY_GRP_XREF_CURR_SMT","PROGRAM","PROPERTY","PROPERTY_DIVISION","PROPERTY_GRP","PROPERTY_GRP_XREF","RATECLASS","SALES_PLAN_SUB_STATUS","SELLING_TITLE","SELLING_TITLE_CAT_GRP","SELLING_TITLE_CAT_GRP_XREF","SELLING_TITLE_GENRE","SELLING_TITLE_GRP","SELLING_TITLE_GRP_TRANS","SELLING_TITLE_GRP_XREF","SELLING_TITLE_GRP_XREF_TRANS","SELLING_TITLE_GRP_XRF_TRS_MSTR","SELLING_TITLE_SET_GRP","SELLING_TITLE_SET_GRP_XREF","SOURCE_SYSTEM","TRAFFIC_LOCK_STATUS","TRAFFIC_STATUS","TRAFFIC_UNIT_STATUS","UNIT_TYPE","V_CDW_CALENDAR_DATE_FISC","V_CDW_CALENDAR_MONTH_FISC","V_CDW_CALENDAR_QTR_FISC","CALENDAR_DATE","CALENDAR_BROADCAST_QUARTER","DEAL_MARKET_GRP_XREF","DEAL_MARKET_GRP","COMMERCIAL_SUB_TYPE","INVENTORY_UNIT_TYPE","INVENTORY_UNIT_TYPE_GRP_XREF","INVENTORY_UNIT_TYPE_GRP","BUSINESS_TYPE","RH_PROPERTY_GRP","RH_BUSINESS_GROUP","RH_BUSINESS_PROPERTY_GRP","RH_LINE_OF_BUSINESS","RH_SELLING_VERTICAL","RH_SUB_DIVISION","RH_DIVISION") ,"sales_detail_fact","sales_option_qtr_fact","smo_week_inventory","traf_inventory_fact") // weekly tables are as a file, load it in a df and do a collect (This approach works only in 68 due to access denied in 67)​
​
​
// val srcTableList=spark.read.option("header","false").option("delimiter","~").csv("/home/206478585/ddlFilesTimeMachine/timeMachineWeeklyTablelist.txt").select("_c0").collect.map(x => x.toString.substring(1,x.toString.length -1).trim)​
​
// all yield the same result.​
//.map(x => x.toString.substring(1,x.toString.length -1).trim) ​
//.map(x => x.toString.substring(x.toString.indexOf("[")+1,x.toString.lastIndexOf("]")).trim) ​
//.map(x => x.toString.replace("[","").replace("]","").trim) // works perfectly in 68​
​
.substring(1, (x.length) -1)​
​
​
import scala.collection.mutable.ArrayBuffer​
​
var finalTableList=ArrayBuffer[String]()​
for (tableName <- tableNameList) // taking ddl for relavant table ​
if(  srcTableList contains tableName.dropRight(4).toUpperCase )​
finalTableList+=tableName​
​
​
import org.apache.spark.sql.DataFrame​
def ddlCreater(DDL : DataFrame,fileAndPAth:String) =DDL.rdd.saveAsTextFile("/home/206478585/ddlTemp/"+fileAndPAth+".table.ddl")​
​
​
var finalTableListFinal=ArrayBuffer[String]()​
for (tableName <- finalTableList)​
{​
val describeOutput=spark.sql(s"describe table time_machine_prod.${tableName}")​
//describeOutput.show​
val colNames=describeOutput.select("col_name").collect.map(x => x.toString.replace("[","").replace("]","").trim)​
var RawDdlStringTemp=RawDdlString+tableName.dropRight(4).toUpperCase+"("​
val RawTableNameTemp=snowflakeSchemaAndDb+"RAW_"+tableName.dropRight(4).toUpperCase​
var CurrDdlStringTemp=CurrDdlString+tableName.dropRight(4).toUpperCase+"("​
val CurrTableNameTemp=snowflakeSchemaAndDb+"CURR_"+tableName.dropRight(4).toUpperCase​
var HistDdlStringTemp=HistDdlString+tableName.dropRight(4).toUpperCase+"("​
val HistTableNameTemp=snowflakeSchemaAndDb+"HIST_"+tableName.dropRight(4).toUpperCase​
​
val numOfCols=colNames.size​
var colCount:Int=0​
if( (srcTableList contains tableName.dropRight(4).toUpperCase ) && !(finalTableListFinal contains tableName.dropRight(4).toUpperCase))​
{​
finalTableListFinal+=tableName.dropRight(4).toUpperCase​
for (colName <- colNames)​
{​
if(!colName.contains("#")  ) // idnoring partition info string​
{​
colCount=colCount+1​
if (colCount==1)​
{​
RawDdlStringTemp=RawDdlStringTemp+colName+" "+varcharDataType​
CurrDdlStringTemp=CurrDdlStringTemp+colName+" "+varcharDataType​
HistDdlStringTemp=HistDdlStringTemp+colName+" "+varcharDataType​
}​
else if (colCount!=numOfCols)​
{​
RawDdlStringTemp=RawDdlStringTemp+","+colName+" "+varcharDataType​
CurrDdlStringTemp=CurrDdlStringTemp+","+colName+" "+varcharDataType​
HistDdlStringTemp=HistDdlStringTemp+","+colName+" "+varcharDataType​
}​
else​
{​
RawDdlStringTemp=RawDdlStringTemp+");"​
CurrDdlStringTemp=CurrDdlStringTemp+" ,ETLLOGID VARCHAR(16777216),SOURCEID VARCHAR(16777216),FILEID VARCHAR(16777216),FILEDT VARCHAR(16777216));"​
HistDdlStringTemp=HistDdlStringTemp+",ETLLOGID VARCHAR(16777216),SOURCEID VARCHAR(16777216),FILEID VARCHAR(16777216),FILEDT VARCHAR(16777216));"​
//println(RawDdlStringTemp) //printing DDL​
//println(CurrDdlStringTemp)​
//println(HistDdlStringTemp)​
DDFListBuffer+=ddlFormatterClass(tableName.dropRight(4),RawDdlStringTemp,CurrDdlStringTemp,HistDdlStringTemp) // DDL generation​
/*​
val rawDDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
rawDDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(RawDdlStringTemp)​
ddlCreater(DDFListBuffer.toSeq.toDF,RawTableNameTemp)​
val currDDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
currDDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(CurrDdlStringTemp)​
ddlCreater(DDFListBuffer.toSeq.toDF,CurrTableNameTemp)​
val histDDFListAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
histDDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(HistDdlStringTemp)​
ddlCreater(DDFListBuffer.toSeq.toDF,HistTableNameTemp) */​
DDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(RawDdlStringTemp)​
DDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(CurrDdlStringTemp)​
DDFListAsSingleColumn+=ddlFormatterClassAsSingleColumn(HistDdlStringTemp)​
}​
}​
}​
}​
}​
​
val DDLDF=DDFListBuffer.toSeq.toDF​
DDLDF.write.mode("overwrite").format("com.databricks.spark.csv").option("header","true").option("delimiter","|").save("/home/206478585/ddlForTimeMachine/")​
​
case class tableNameAndCount(tableName:String,count:Int)​
​
val countListBuffer:ListBuffer[tableNameAndCount]= new ListBuffer[tableNameAndCount]()​
​
for (tableName <- finalTableList)​
{​
if( (srcTableList contains tableName.dropRight(4).toUpperCase ) && (finalTableListFinal contains tableName.dropRight(4).toUpperCase)) // already added in previous loop. so just schecking​
{​
val countDf=spark.sql("select count(*) as count from time_machine_prod."+tableName)​
val countForTable=countDf.select("count").collect.map(_.toString.replace("[","").replace("]","").trim)​
countListBuffer+=tableNameAndCount(tableName.dropRight(4).toUpperCase,countForTable(0).toInt)​
}​
}​
​
val countDF=countListBuffer.toSeq.toDF.distinct​
​
countDF.orderBy("tableName").show(100,false)​
​
// sorting and checking all are correct​
finalTableListFinal.sortWith(_ < _) sameElements srcTableList.sortWith(_ < _)​
​
​
val distinctTableList=DDLDF.select("tableName").distinct.collect.map(x => x.toString.replace("[","").replace("]","").trim)​
​
distinctTableList foreach println​
​
// for RDS Insert statements​
val srcTableList=Array("ADVERTISER","ADVERTISER_GRP_CURR_SMT","ADVERTISER_GRP_XREF_CURR_SMT","AGENCY","AGENCY_GRP_CURR_SMT","AGENCY_GRP_XREF_CURR_SMT","CDW_CALENDAR_DATE","CDW_CALENDAR_MONTH","CDW_CALENDAR_QTR","CDW_CALENDAR_SEASON_YEAR","CDW_CALENDAR_TYPE","CDW_CALENDAR_WEEK","CDW_CALENDAR_YEAR","DAYPART","DAYPART_GRP_MSTR","DAYPART_GRP_XREF_MSTR","DEAL_MARKET","DEAL_STATUS","DEAL_STATUS_GRP","DEAL_STATUS_GRP_XREF","DEAL_TYPE","DEAL_TYPE_GRP","DEAL_TYPE_GRP_XREF","DEAL_USR","DEMO","DEMO_GRP","DEMO_GRP_XREF","EPISODE","EVENT_TYPE","INDUSTRY_CATEGORY","INDUSTRY_CATEGORY_GRP_SMT","INDUSTRY_CATEGORY_GRP_XREF_SMT","INDUSTRY_SUB_CATEGORY","INDUSTRY_SUB_SUB_CATEGORY","KEY_PROPERTY_DAYPART","PARENT_AGENCY_GRP_CURR_SMT","PARENT_AGY_GRP_XREF_CURR_SMT","PROGRAM","PROPERTY","PROPERTY_DIVISION","PROPERTY_GRP","PROPERTY_GRP_XREF","RATECLASS","SALES_PLAN_SUB_STATUS","SELLING_TITLE","SELLING_TITLE_CAT_GRP","SELLING_TITLE_CAT_GRP_XREF","SELLING_TITLE_GENRE","SELLING_TITLE_GRP","SELLING_TITLE_GRP_TRANS","SELLING_TITLE_GRP_XREF","SELLING_TITLE_GRP_XREF_TRANS","SELLING_TITLE_GRP_XRF_TRS_MSTR","SELLING_TITLE_SET_GRP","SELLING_TITLE_SET_GRP_XREF","SOURCE_SYSTEM","TRAFFIC_LOCK_STATUS","TRAFFIC_STATUS","TRAFFIC_UNIT_STATUS","UNIT_TYPE","V_CDW_CALENDAR_DATE_FISC","V_CDW_CALENDAR_MONTH_FISC","V_CDW_CALENDAR_QTR_FISC","CALENDAR_DATE","CALENDAR_BROADCAST_QUARTER","DEAL_MARKET_GRP_XREF","DEAL_MARKET_GRP","COMMERCIAL_SUB_TYPE","INVENTORY_UNIT_TYPE","INVENTORY_UNIT_TYPE_GRP_XREF","INVENTORY_UNIT_TYPE_GRP","BUSINESS_TYPE","RH_PROPERTY_GRP","RH_BUSINESS_GROUP","RH_BUSINESS_PROPERTY_GRP","RH_LINE_OF_BUSINESS","RH_SELLING_VERTICAL","RH_SUB_DIVISION","RH_DIVISION")​
​
var tblCount=0​
var jobId=2990​
for (distinctTableLower <- distinctTableList)​
{​
val distinctTable=distinctTableLower.toUpperCase​
if ( srcTableList contains distinctTable )​
{​
val insertQueryJobInfo=s"INSERT INTO data_integration_dev.es_job_info values (%jobId%,'TIME_MACHINE_WEEKLY_${distinctTable}',299,'TIME_MACHINE_WEEKLY','load.run_copy','TIME_MACHINE_WEEKLY_${distinctTable}',0,'N','N','Y','206643136',CURRENT_TIMESTAMP,'206643136',CURRENT_TIMESTAMP);"​
​
println(insertQueryJobInfo.replace("%jobId%",jobId.toString))​
​
val insertQueryFileInfo=s"INSERT INTO data_integration_dev.es_File_Info values ('%jobId%','299','TIME_MACHINE_WEEKLY_${distinctTable}','COPY_TIME_MACHINE_WEEKLY_${distinctTable}','AWS','bigdata/iam/bigdata_batch_user','NBCU_DATA_INTEG_TIME_MACHINE_PROD_WH1','NBCU_DATA_INTEG_META_PROD','TIME_MACHINE','RAW_${distinctTable}','s3://nbcu-use1-data-analytics-dropzone/dropzone_TimeMachine/${distinctTable}',NULL,'.*r*','CSV_TIME_MACHINE_WEEKLY',NULL,'truncate table TIME_MACHINE.CURR_${distinctTable}  '||chr(59)||'  insert into TIME_MACHINE.CURR_${distinctTable}  select x.*,@ETLLOGID, @SOURCEID, @FILEID, @DATE_YY  from TIME_MACHINE.RAW_${distinctTable} x '||chr(59)||' insert into TIME_MACHINE.HIST_${distinctTable}  select x.* from TIME_MACHINE.CURR_${distinctTable} x '||chr(59)||'  ','TRUNCATE TABLE  TIME_MACHINE.RAW_${distinctTable} ','Y','206643136',CURRENT_TIMESTAMP,'206643136',CURRENT_TIMESTAMP);"​
​
println(insertQueryFileInfo.replace("%jobId%",jobId.toString))​
jobId=jobId+1​
tblCount=tblCount+1​
}​
}​
​
​
​
// count (*) statements for snowflake View creation ​
​
var unionString=""​
val srcTableListSize=srcTableList.size​
var counter=0​
for ( srcTable <- srcTableList )​
{​
//println(s"select count(*) from "+snowflakeSchemaAndDb+"HIST_"+srcTable+";")​
//println(s"select * from "+snowflakeSchemaAndDb+"CURR_"+srcTable+";")​
//println(s"select count(*) from "+snowflakeSchemaAndDb+"HIST_"+srcTable+";")​
//println(s"select * from "+snowflakeSchemaAndDb+"CURR_"+srcTable+";")​
counter=counter+1​
if (counter==srcTableListSize)​
{​
unionString=unionString+s"select 'CURR_"+srcTable+"' as tableName,count(*) as rowCount from "+snowflakeSchemaAndDb+"CURR_"+srcTable+" union all "​
unionString=unionString+s"select 'HIST_"+srcTable+"' as tableName,count(*) as rowCount  from "+snowflakeSchemaAndDb+"HIST_"+srcTable+" ; "​
}​
else​
{​
unionString=unionString+s"select 'CURR_"+srcTable+"' as tableName,count(*) as rowCount  from "+snowflakeSchemaAndDb+"CURR_"+srcTable+" union all "​
unionString=unionString+s"select 'HIST_"+srcTable+"' as tableName,count(*) as rowCount  from "+snowflakeSchemaAndDb+"HIST_"+srcTable+" union all "​
}​
}​
​
for (srcTable <- srcTableList )​
{​
println("truncate table "+snowflakeSchemaAndDb+"HIST_"+srcTable+";")​
println("truncate table "+snowflakeSchemaAndDb+"RAW_"+srcTable+";")​
println("truncate table "+snowflakeSchemaAndDb+"CURR_"+srcTable+";")​
}​
​
for (srcTable <- srcTableList )​
println(snowflakeSchemaAndDb+"HIST_"+srcTable)​
-------------------------------------​
​
// for drop tble ​
​
val RawDropDdlString:String="DROP TABLE "+snowflakeSchemaAndDb+"RAW_"​
val CurrDropDdlString:String="DROP TABLE "+snowflakeSchemaAndDb+"CURR_"​
val HistDropDdlString:String="DROP TABLE "+snowflakeSchemaAndDb+"HIST_"​
​
for (tableName <- tableNameList)​
{​
println(RawDropDdlString+tableName.dropRight(4).toUpperCase+";")​
println(CurrDropDdlString+tableName.dropRight(4).toUpperCase+";")​
println(HistDropDdlString+tableName.dropRight(4).toUpperCase+";")​
}​
​
------------------------- ​
DDL file creation​
​
24 -- starting of NBCU ​
​
case class ddlFormatterClassAsSingleColumn(tableDDL:String)​
​
val ddlDF=spark.read.option("delimiter","~").csv("/home/206478585/ddlFilesTimeMachine/ddlList.txt")​
//val ddlDF=spark.read.option("delimiter","~").csv("/home/206478585/ddlTemp/ddlList.txt")​
val ddlList=ddlDF.select("_c0").collect.map(_.toString.replace("[","").replace("]","").trim)​
​
for (ddl <- ddlList)​
{​
val DDLAsSingleColumn:ListBuffer[ddlFormatterClassAsSingleColumn]= new ListBuffer[ddlFormatterClassAsSingleColumn]()​
DDLAsSingleColumn+=ddlFormatterClassAsSingleColumn(ddl)​
DDLAsSingleColumn.toSeq.toDF.coalesce(1).write.mode("overwrite").option("header","false").option("delimiter","~").csv("/home/206478585/ddlTemp/"+ddl.substring(24,ddl.indexOf("("))+".table.ddl")​
}​
​
import sys.process._​
val basePath="/home/206478585/ddlTemp"​
val basePathOutput=basePath.substring(0,basePath.lastIndexOf("/"))+"/ddlFilesTimeMachine/"​
val fileName="ls "+basePath+"/"!!​
val fileNameArray=fileName.split("\n")​
for(fileNameElement <-fileNameArray)​
{​
val filesInside="ls "+basePath+"/"+fileNameElement!!​
val filesInsideList=filesInside.split("\n").sortWith(_<_)​
"mv "+basePath+"/"+fileNameElement+"/"+filesInsideList(1)+" "+basePathOutput+fileNameElement!​
}​
​
/*​
view ddl​
​
create or replace view "NBCU_DATA_INTEG_META_PROD"."TIME_MACHINE".STATSTIMEMACHINEWEEKLY_count_STATS as select  a.TableName as TeraDataTableName,​
b.tableName, a.rowCount as TeraDAtaTableCount,​
b.rowCount as DropzoneTableCount, case when a.rowCount = b.rowCount then 'MATCHING' else 'NOT-MATCHING' end as countMatchStats,case when a.rowCount > b.rowCount then 'TERA DATA HAS MORE' ​
when a.rowCount < b.rowCount then 'DROP ZONE HAS MORE'  else 'BOTH HAVE SAME AMOUNT OF RECORDS' end as countMatchStatsComments,case when a.rowCount > b.rowCount then a.rowCount-b.rowCount ​
when a.rowCount < b.rowCount then b.rowCount-a.rowCount else 0 end as countMismatch from "NBCU_DATA_INTEG_META_PROD"."TIME_MACHINE"."STATSTIMEMACHINEWEEKLY_TERADATA_LOAD" a ​
join "NBCU_DATA_INTEG_META_PROD"."TIME_MACHINE"."STATSTIMEMACHINEWEEKLY" b on a.actualTableName=b.TABLENAME;​
​
*/​
​
​
