

import org.apache.spark.sql.DataFrame



val srcDF=spark.read.format("com.databricks.spark.csv").option("delimiter","\001").option("header","true").option("inferSchema","true").option("basePath","s3a://nbcu-backup/data/clean/time_machine/youtube_tv/daily/daily_events/").load("s3a://nbcu-backup/data/clean/time_machine/youtube_tv/daily/daily_events/brand=bravo/filedate=20170405")



val destDF=spark.read.format("com.databricks.spark.csv").option("delimiter","\001").option("header","true").option("inferSchema","false").option("basePath","s3a://nbcu-backup/data/clean/time_machine/youtube_tv/daily/daily_events/").load("s3a://nbcu-backup/data/clean/time_machine/youtube_tv/daily/daily_events/brand=chiller/filedate=20170405")



//val destDF=spark.read.format("com.databricks.spark.csv").option("delimiter","\001").option("header","true").option("inferSchema","true").option("basePath","s3a://nbcu-backup/data/clean/time_machine/youtube_tv/daily/daily_events/").load("s3a://nbcu-backup/data/clean/time_machine/youtube_tv/daily/daily_events/brand=chiller/filedate=20170405")



//val destDF=spark.read.format("com.databricks.spark.csv").option("delimiter","\001").option("header","true").option("inferSchema","true").load("s3a://nbcu-backup/data/clean/time_machine/youtube_tv/daily/daily_events/brand=chiller/filedate=20170405")



def dfSchemaChecker(dfSrc:DataFrame,dfDest:DataFrame)=

{

val dfSchemaSrc=dfSrc.dtypes.map(x  =>x.toString).map(x=>x.toString.substring(1,x.toString.length-1))

val dfSchemaDest=dfDest.dtypes.map(x  =>x.toString).map(x=>x.toString.substring(1,x.toString.length-1))



val srcSchemaMap:collection.mutable.LinkedHashMap[String,String]= collection.mutable.LinkedHashMap[String,String]()

val destSchemaMap:collection.mutable.LinkedHashMap[String,String]= collection.mutable.LinkedHashMap[String,String]()



for(dfTempSchemaSrc <- dfSchemaSrc)

srcSchemaMap.put(dfTempSchemaSrc.split(",",2)(0),dfTempSchemaSrc.split(",",2)(1)) //column,dtype



for(dfTempSchemaDest <- dfSchemaDest)

destSchemaMap.put(dfTempSchemaDest.split(",",2)(0),dfTempSchemaDest.split(",",2)(1)) //column,dtype



val colNamesSrc=srcSchemaMap.keys.toArray

val colNamesDest=destSchemaMap.keys.toArray

if (colNamesDest.size == colNamesSrc.size)

{

var resultColCount:Int=0

val numOfColumns=colNamesDest.size // colNamesSrc.size

for (i <- 0 to numOfColumns -1)

{

if (colNamesSrc(i).toLowerCase == colNamesDest(i).toLowerCase) // col name match

if(srcSchemaMap(colNamesSrc(i)).toLowerCase == destSchemaMap(colNamesDest(i)).toLowerCase )  // schema match

resultColCount=resultColCount+1

}

resultColCount match {case value if value == numOfColumns => println("Schema matched") case _ => println("Schema did not match") }

}

else

println("Number of colum's did not match")



}



dfSchemaChecker(srcDF,destDF)



