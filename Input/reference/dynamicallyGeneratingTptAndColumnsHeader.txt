import scala.collection.mutable.ListBuffer

import scala.collection.mutable.ArrayBuffer



val tableList=spark.sql("show tables in time_machine_prod")

val tableNameList=tableList.select("tableName").collect.map(x => x.toString.replace("[","").replace("]","").trim)



case class selectQueryClass(selectSql:String,tableName:String)

case class tableHeader(header:String,tableName:String)



val srcTableList=Array("ADVERTISER","ADVERTISER_GRP_CURR_SMT","ADVERTISER_GRP_XREF_CURR_SMT","AGENCY","AGENCY_GRP_CURR_SMT","AGENCY_GRP_XREF_CURR_SMT","CDW_CALENDAR_DATE","CDW_CALENDAR_MONTH","CDW_CALENDAR_QTR","CDW_CALENDAR_SEASON_YEAR","CDW_CALENDAR_TYPE","CDW_CALENDAR_WEEK","CDW_CALENDAR_YEAR","DAYPART","DAYPART_GRP_MSTR","DAYPART_GRP_XREF_MSTR","DEAL_MARKET","DEAL_STATUS","DEAL_STATUS_GRP","DEAL_STATUS_GRP_XREF","DEAL_TYPE","DEAL_TYPE_GRP","DEAL_TYPE_GRP_XREF","DEAL_USR","DEMO","DEMO_GRP","DEMO_GRP_XREF","EPISODE","EVENT_TYPE","INDUSTRY_CATEGORY","INDUSTRY_CATEGORY_GRP_SMT","INDUSTRY_CATEGORY_GRP_XREF_SMT","INDUSTRY_SUB_CATEGORY","INDUSTRY_SUB_SUB_CATEGORY","KEY_PROPERTY_DAYPART","PARENT_AGENCY_GRP_CURR_SMT","PARENT_AGY_GRP_XREF_CURR_SMT","PROGRAM","PROPERTY","PROPERTY_DIVISION","PROPERTY_GRP","PROPERTY_GRP_XREF","RATECLASS","SALES_PLAN_SUB_STATUS","SELLING_TITLE","SELLING_TITLE_CAT_GRP","SELLING_TITLE_CAT_GRP_XREF","SELLING_TITLE_GENRE","SELLING_TITLE_GRP","SELLING_TITLE_GRP_TRANS","SELLING_TITLE_GRP_XREF","SELLING_TITLE_GRP_XREF_TRANS","SELLING_TITLE_GRP_XRF_TRS_MSTR","SELLING_TITLE_SET_GRP","SELLING_TITLE_SET_GRP_XREF","SOURCE_SYSTEM","TRAFFIC_LOCK_STATUS","TRAFFIC_STATUS","TRAFFIC_UNIT_STATUS","UNIT_TYPE","V_CDW_CALENDAR_DATE_FISC","V_CDW_CALENDAR_MONTH_FISC","V_CDW_CALENDAR_QTR_FISC","CALENDAR_DATE","CALENDAR_BROADCAST_QUARTER","DEAL_MARKET_GRP_XREF","DEAL_MARKET_GRP","COMMERCIAL_SUB_TYPE","INVENTORY_UNIT_TYPE","INVENTORY_UNIT_TYPE_GRP_XREF","INVENTORY_UNIT_TYPE_GRP","BUSINESS_TYPE","RH_PROPERTY_GRP","RH_BUSINESS_GROUP","RH_BUSINESS_PROPERTY_GRP","RH_LINE_OF_BUSINESS","RH_SELLING_VERTICAL","RH_SUB_DIVISION","RH_DIVISION","SALES_DETAIL_FACT","SALES_OPTION_QTR_FACT","SMO_WEEK_INVENTORY","TRAF_INVENTORY_FACT")











val sqlBuffer:ListBuffer[selectQueryClass]=ListBuffer[selectQueryClass]()

var finalTableListFinal=ArrayBuffer[String]()

val headerBuffer:ListBuffer[tableHeader]=ListBuffer[tableHeader]()



var finalTableList=ArrayBuffer[String]()

for (tableName <- tableNameList) // taking ddl for relavant table

if(  srcTableList contains tableName.dropRight(4).toUpperCase )

finalTableList+=tableName



for (tableName <- finalTableList)

{

val describeOutput=spark.sql(s"describe table time_machine_prod.${tableName}")

//describeOutput.show

val colNames=describeOutput.select("col_name").collect.map(x => x.toString.replace("[","").replace("]","").trim)

var selectQuery="select "

var headerTxt=""

val numOfCols=colNames.size

var colCount:Int=0

if( (srcTableList contains tableName.dropRight(4).toUpperCase ) && !(finalTableListFinal contains tableName.dropRight(4).toUpperCase))

{

finalTableListFinal+=tableName.dropRight(4).toUpperCase

for (colName <- colNames)

{

if(!colName.contains("#")  ) // ignoring partition info string

{

colCount=colCount+1

if (colCount==1)

{

selectQuery=selectQuery+" cast("+colName+" as varchar(500))"

headerTxt=headerTxt+colName

}

else if (colCount!=numOfCols)

{

selectQuery=selectQuery+" , cast("+colName+" as varchar(500)) "

headerTxt=headerTxt+"|"+colName

}

else

{

selectQuery=selectQuery+" , cast("+colName+" as varchar(500)) from "+tableName.dropRight(4).toUpperCase+";"

headerTxt=headerTxt+colName

}

}

}

sqlBuffer+=selectQueryClass(selectQuery,tableName.dropRight(4).toUpperCase)

headerBuffer+=tableHeader(headerTxt,tableName.dropRight(4).toUpperCase)

}

}



sqlBuffer.toSeq.toDF.distinct.show(200,false)

headerBuffer.toSeq.toDF.distinct.show(200,false)



import scala.collection.mutable.ListBuffer



val headerDF=spark.read.option("inferSchema","true").option("header","true").option("delimiter","~").csv("file:///home/206478585/temp/timeMachineHeader.txt")

val sqlDF=spark.read.option("inferSchema","true").option("header","true").option("delimiter","|").csv("file:///home/206478585/temp/timeMachineSelect.txt")



case class selectQuerySingle(selectSql:String)

case class tableHeaderSingle(header:String)







val finalTableList=sqlDF.select("tableName").collect.map(x => x.toString.substring(1,x.toString.length -1).trim)

val tptText="USING CHARACTER SET ASCII DEFINE JOB EXPORT_TO_FILE  DESCRIPTION 'EXPORT TIME MACHINE Data Set'  (DEFINE SCHEMA %tableName% FROM SELECT OF OPERATOR EX_%tableName%;  DEFINE OPERATOR EX_%tableName% TYPE EXPORT SCHEMA %tableName%   ATTRIBUTES ( UserName = @USERNAME ,UserPassword = @PWORD,TdpId = @ENVNAME||'.'||@VAR||'TFAYD.COM',MaxSessions= 2,MinSessions= 2      ,SpoolMode='NoSpool',QueryBandSessInfo='UtilityDataSize=Large;',SelectStmt = '%query%');  DEFINE OPERATOR WR_%tableName%   TYPE DATACONNECTOR CONSUMER  SCHEMA %tableName%  ATTRIBUTES (FileName = '/infdevwc_data/DST/TgtFiles/DST/TIME_MACHINE/%tableName%.txt', Format = 'DELIMITED',TextDelimiter='|',  QuotedData = 'Optional',IndicatorMode = 'N',OpenMode = 'Write'); APPLY TO OPERATOR (WR_%tableName%[1]) SELECT *  FROM OPERATOR (EX_%tableName%[10]); ) ; "

for(tableName <- finalTableList)

{

val header = headerDF.filter("trim(tableName) = '"+tableName+"'").select("header").collect.map(x => x.toString.substring(1,x.toString.trim.length -1).trim)

val selectQuery = sqlDF.filter("trim(tableName) = '"+tableName+"'").select("selectSql").collect.map(x => x.toString.substring(1,x.toString.length -1).trim)

val sqlBufferSingle:ListBuffer[selectQuerySingle]=ListBuffer[selectQuerySingle]()

val headerSingle:ListBuffer[tableHeaderSingle]=ListBuffer[tableHeaderSingle]()

sqlBufferSingle+=selectQuerySingle(tptText.replace("%query%",selectQuery(0)).replace("%tableName%",tableName))// actually tptString

headerSingle+=tableHeaderSingle(header(0))

headerSingle.toSeq.toDF.coalesce(1).write.mode("overwrite").option("quoteMode", "NON_NUMERIC").option("quote", "\"").option("header","false").csv("file:///home/206478585/temp/timeMachineTemp/"+tableName+"_HEADER.txt")

sqlBufferSingle.toSeq.toDF.coalesce(1).write.mode("overwrite").option("quoteMode", "NON_NUMERIC").option("quote", "\"").option("header","false").csv("file:///home/206478585/temp/timeMachineTemp/"+tableName+".tpt")

}



import sys.process._





val paths="ls /home/206478585/temp/timeMachineTemp/"!!

val pathsReal=paths.split("\n")

val basePath="/home/206478585/temp/timeMachineTemp/"

val outputBasePath="/home/206478585/temp/timeMachineOutput/"

for (path <- pathsReal)

{

"rm "+basePath+path+"/_SUCCESS"!;

val fileNamePath="ls "+basePath+path!!;

"cp "+basePath+path+"/"+fileNamePath.split("\n")(0)+" "+outputBasePath+path!

}







