

spark-shell --packages io.delta:delta-core_2.11:0.4.0

/home/raptor/Softwares/hadoop-2.7.3/sbin/start-dfs.sh

hdfs dfs -mkdir -p /user/raptor/testing/hadoop/

// using existing one

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/deltaTablePartitioned /user/raptor/testing/hadoop/

val deltaTable=spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTablePartitioned/")

val deltaTableDeltaVersion=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTablePartitioned/")
// history only works on dellta table class.

// timetravel

val deltaTableVersion=spark.read.format("delta").option("asOfVersion","3").load("/user/raptor/testing/hadoop/deltaTablePartitioned/")

val deltaTableVersion=spark.read.format("delta").option("asOfTimestamp","").load("/user/raptor/testing/hadoop/deltaTablePartitioned/")


// creating new one 

hdfs dfs -mkdir -p /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car3.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_cars4.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car4.txt

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car2.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_diff_schema.txt

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_Car_ExtraColumn.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_ExtraColumn_schema.txt

// loading to DF

val deltaTableInput1=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car3.txt").selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time")

val deltaTableInput2=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car2.txt")

val deltaTableInput3=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car4.txt").selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time")

val deltaTableInputDiffSchema=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_diff_schema.txt")

val deltaTableInputExtraColumn=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_ExtraColumn_schema.txt").selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time","number_of_owners")

// converting from dd-mm-YYYY HH:mm:ss to YYYY-MM-DD HH:mm:ss

// changed at top.
val tempDeltaTableInput1= deltaTableInput1.selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time")

// partition by not working in shell(2.11, scla, working in 2.12 scala in intellji) -- write through intelliji
deltaTableInput1.write.format("delta").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_tempCheck_1")

// loaded delta table

val deltaTableUpdated=spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")

deltaTableInput1.write.mode("overwrite").partitionBy("brand","model","year","month").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze")

deltaTableInput2.write.mode("append").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze") // ->>> not working, instead use delta table 
deltaTableDeltaFormat.as("HIST").merge(deltaTableInput2.as("CURR"),"HIST.Vehicle_id = CURR.Vehicle_id").whenMatched().updateAll().whenNotMatched().insertAll().execute()

deltaTableInput3.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze")

deltaTableDeltaFormat_bronze.as("HIST").merge(deltaTableInput2.as("CURR"),"HIST.Vehicle_id = CURR.Vehicle_id").whenMatched().updateAll().whenNotMatched().insertAll().execute()

deltaTableInputDiffSchema.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned") // wont work Diff Schema

deltaTableInputDiffSchema.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").option("mergeSchema","true").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned") // wont work schema with more columns can only be merged, less columns cannot be meged

deltaTableInputExtraColumn.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").option("mergeSchema","true").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")// works

val deltaTableDeltaFormat=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")

val deltaTableDeltaFormat_bronze=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze")

deltaTableUpdated.select("model").distinct.show

// adding another Column.

deltaTableDeltaFormat_bronze.toDF.selectExpr("Vehicle_id","model","brand","year","month","miles","intake_date_time","case when brand in ('Ford','Lambhorghini') then cast (1 as INT) else cast(2 as int) end  as number_of_owners ").write.mode("overwrite").format("delta").option("spark.sql.sources.partitionOverwriteMode","dynamic").save("/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_broze_extraColumn")

val deltaTableUpdated=spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")

deltaTableInputExtraColumn

deltaTableDeltaFormat.updateExpr("model = 'Eco-Sport'",Map("model"->"'Free-Style'","year"->"2017")) // string in single quoutes in update expr

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned").where("brand in  ('Volswogen','Renault','Ferrari','Aston-Martin')")
('Volswogen','Renault','Ferrari','Aston-Martin') // taking new data

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned").filter("brand not in  ('Volswogen','Renault','Ferrari','Aston-Martin')")
//('Volswogen','Renault','Ferrari','Aston-Martin')

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned").where("brand not in  ('Volswogen','Renault','Ferrari','Aston-Martin')").withColumn("number_of_owners",expr("coalesce(number_of_owners,case when brand in ('Ford','Lambhorghini') then cast (1 as INT) else cast(2 as int) end )"))


cd /home/raptor/IdeaProjects/SparkLearning/build/libs/

spark-submit --class org.controller.deltaLakeEG.deltaLakeHadoopEg --deploy-mode client --master yarn --num-executors 2 --executor-memory 1g --driver-memory 1g --driver-cores 2 --executor-cores 2   SparkLearning-1.0-SNAPSHOT.jar





