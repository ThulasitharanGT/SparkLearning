

spark-shell --packages io.delta:delta-core_2.11:0.5.0
spark-shell --packages io.delta:delta-core_2.12:0.4.0

/home/raptor/Softwares/hadoop-2.7.3/sbin/start-dfs.sh

hdfs dfs -mkdir -p /user/raptor/testing/hadoop/

// using existing one

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/deltaTablePartitioned /user/raptor/testing/hadoop/

val deltaTable=spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTablePartitioned/")

val deltaTableDeltaVersion=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTablePartitioned/")
// history only works on dellta table class.

// timetravel

val deltaTableVersion=spark.read.format("delta").option("asOfVersion","3").load("/user/raptor/testing/hadoop/deltaTablePartitioned/")

val deltaTableVersion=spark.read.format("delta").option("asOfTimestamp","").load("/user/raptor/testing/hadoop/deltaTablePartitioned/")


// creating new one 

hdfs dfs -mkdir -p /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car3.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_cars4.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car4.txt

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car5.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car5.txt

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car2.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_diff_schema.txt

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_Car_ExtraColumn.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_ExtraColumn_schema.txt

hdfs dfs -put /home/raptor/IdeaProjects/SparkLearning/Input/Avail_car6.txt /user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car6.txt


// loading to DF

val deltaTableInput1=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car3.txt").selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time")

val deltaTableInput2=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car2.txt")

val deltaTableInput3=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car4.txt").selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time")

val deltaTableInputDiffSchema=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_diff_schema.txt")

val deltaTableInputExtraColumn=spark.read.format("com.databricks.spark.csv").option("header","true").option("delimiter","|").option("inferSchema","true").load("/user/raptor/testing/hadoop/deltaTableTestFolder/inputFiles/Avail_car_ExtraColumn_schema.txt").selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time","number_of_owners")

// converting from dd-mm-YYYY HH:mm:ss to YYYY-MM-DD HH:mm:ss

// changed at top.
val tempDeltaTableInput1= deltaTableInput1.selectExpr("Vehicle_id","model","brand","year","month","miles","CAST(concat(substring(intake_date_time,7,4),concat(substring(intake_date_time,3,4),concat(substring(intake_date_time,1,2),substring(intake_date_time,11,9)))) AS TIMESTAMP) as intake_date_time")

// partition by not working in shell(2.11, scla, working in 2.12 scala in intellji) -- write through intelliji
deltaTableInput1.write.format("delta").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_tempCheck_1")

deltaTableInput1.write.mode("overwrite").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze")

DeltaTable.convertToDelta(spark,"parquet.`/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze`","brand string,model string,year int,month int")

// loaded delta table

val deltaTableUpdated=spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")

deltaTableInput1.write.mode("overwrite").partitionBy("brand","model","year","month").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/partitionedDeltaEg")

deltaTableInput2.write.mode("append").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze") // ->>> not working, instead use delta table 
deltaTableDeltaFormat.as("HIST").merge(deltaTableInput2.as("CURR"),"HIST.Vehicle_id = CURR.Vehicle_id").whenMatched().updateAll().whenNotMatched().insertAll().execute()

deltaTableInput3.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze")

deltaTableDeltaFormat_bronze.as("HIST").merge(deltaTableInput2.as("CURR"),"HIST.Vehicle_id = CURR.Vehicle_id").whenMatched().updateAll().whenNotMatched().insertAll().execute()

deltaTableInputDiffSchema.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned") // wont work Diff Schema

deltaTableInputDiffSchema.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").option("mergeSchema","true").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned") // wont work schema with more columns can only be merged, less columns cannot be meged

deltaTableInputExtraColumn.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").option("mergeSchema","true").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")// works

val deltaTableDeltaFormat=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")

val deltaTableDeltaFormat_bronze=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze")

deltaTableUpdated.select("model").distinct.show

// adding another Column.

deltaTableDeltaFormat_bronze.toDF.selectExpr("Vehicle_id","model","brand","year","month","miles","intake_date_time","case when brand in ('Ford','Lambhorghini') then cast (1 as INT) else cast(2 as int) end  as number_of_owners ").write.mode("overwrite").format("delta").option("spark.sql.sources.partitionOverwriteMode","dynamic").save("/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_broze_extraColumn")

val deltaTableUpdated=spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned")

deltaTableInputExtraColumn

deltaTableDeltaFormat.updateExpr("model = 'Eco-Sport'",Map("model"->"'Free-Style'","year"->"2017")) // string in single quoutes in update expr

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned").where("brand in  ('Volswogen','Renault','Ferrari','Aston-Martin')")
('Volswogen','Renault','Ferrari','Aston-Martin') // taking new data

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned").filter("brand not in  ('Volswogen','Renault','Ferrari','Aston-Martin')")
//('Volswogen','Renault','Ferrari','Aston-Martin')

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/temp_DeltaTablePartitioned").where("brand not in  ('Volswogen','Renault','Ferrari','Aston-Martin')").withColumn("number_of_owners",expr("coalesce(number_of_owners,case when brand in ('Ford','Lambhorghini') then cast (1 as INT) else cast(2 as int) end )"))


cd /home/raptor/IdeaProjects/SparkLearning/build/libs/

spark-submit --class org.controller.deltaLakeEG.deltaLakeHadoopEg --deploy-mode client --master yarn --num-executors 2 --executor-memory 1g --driver-memory 1g --driver-cores 2 --executor-cores 2   SparkLearning-1.0-SNAPSHOT.jar

//////////////////////////

val deltaTableDeltaFormat_bronze=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze")

deltaTableInput1.write.mode("overwrite").partitionBy("brand","model","year","month").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze_columnAdditionTry")

deltaTableInput2.write.mode("append").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze") // ->>> not working, instead use delta table 

deltaTableDeltaFormat_bronze.as("HIST").merge(deltaTableInput3.as("CURR"),"HIST.Vehicle_id = CURR.Vehicle_id").whenMatched().updateAll().whenNotMatched().insertAll().execute()

deltaTableInput3.write.mode("overwrite").option("spark.sql.sources.partitionOverwriteMode","dynamic").format("delta").save("hdfs://localhost/user/raptor/testing/hadoop/deltaTableTestFolder/deltaTablePartitioned_bronze")

// merging schema

deltaTableInput1.write.mode("overwrite").partitionBy("brand","model","year","month").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze_columnAdditionTry") // partition by only works on intelliji, yet to try spark 3

deltaTableInput2.write.mode("append").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze_columnAdditionTry") 

deltaTableInput3.write.mode("append").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze_columnAdditionTry") 

deltaTableInput4.write.mode("append").format("delta").oprion("mergeSchema","true").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze_columnAdditionTry") 

val DeltaTableNewTry=DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTableAddingColumnInBetween_Bronze")

DeltaTableNewTry.toDF.show
DeltaTableNewTry.toDF.filter("brand not in ('Volswogen','Ferrari','Aston-Martin','Renault')").show

//moving to gold

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTableAddingColumnInBetween_Bronze").where("brand not in  ('Volswogen','Renault','Ferrari','Aston-Martin')").withColumn("number_of_owners",expr("coalesce(number_of_owners,case when brand in ('Ford','Lambhorghini') then cast (1 as INT) else cast(2 as int) end )")).write.mode("overwrite").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTableAddingColumnInBetween_Silver")

spark.read.format("delta").load("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTableAddingColumnInBetween_Bronze").where("brand in  ('Volswogen','Renault','Ferrari','Aston-Martin')").write.mode("append").format("delta").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTableAddingColumnInBetween_Silver")

.option("overwriteSchema", "true") --> for overwriting schema

.option("mergeSchema", "true")  --> for addding columns



----------------------------

temp try 

val deltaTableSilver=DeltaTable.forPath(spark,outputPath+"carsDeltaTable_Silver")

val deltaTableBronze=DeltaTable.forPath(spark,outputPath+"carsDeltaTable_Bronze")

deltaTableBronze.as("bronze").merge(deltaInput3.as("df"),"bronze.Vehicle_id=df.Vehicle_id").whenMatched.updateAll.whenNotMatched.insertAll.execute()

deltaTableBronze.as("bronze").merge(deltaInput2.as("df"),"bronze.Vehicle_id=df.Vehicle_id and bronze.model=df.model").whenMatched.updateExpr(Map("model"->"bronze.model","brand"->"bronze.brand","year" -> "bronze.year" ,"month"->"bronze.month","miles"->"bronze.miles")).whenNotMatched.insertAll.execute()


deltaTableBronze.as("bronze").merge(deltaTableSilver.toDF.as("silver"),"bronze.Vehicle_id=silver.Vehicle_id").whenMatched.updateExpr(Map("model"->"bronze.model","brand"->"bronze.brand","year" -> "bronze.year" ,"month"->"bronze.month","miles"->"bronze.miles")).whenNotMatched.insertAll.execute()

-------------------------------

creating pipeline

deltaTableInput1.write.mode("overwrite").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze") // not partitioning while we write as delta, so converting it to delta // can be said like historic table is coverted to delta table

DeltaTable.convertToDelta(spark,"parquet.`/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze`","brand string,model string,year int,month int")

deltaTableInput2.write.format("delta").mode("append").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze")

deltaTableInput3.write.format("delta").mode("append").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze")

// actually replace where must come for historic data like less data year time. like less than 2010 data had one column less than new data. then we can replicate that data in a new df and with column the new column and overwrite using replace where --> here i took brand badexample but same functionality

deltaTableInputExtraColumn.write.format("delta").mode("append").option("mergeSchema","true").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze")

//replacing old data with additional coloumn value

import org.apache.spark.sql.functions._

DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze").toDF.filter("brand not in ('Ferrari','Volswogen','Renault','Aston-Martin')").drop("number_of_owners").withColumn("number_of_owners",when(col("brand") ==="Hyundai",2).when(col("brand") ==="Lambhorghini",5).otherwise(3)).write.mode("overwrite").format("delta").option("replaceWhere","brand not in ('Ferrari','Volswogen','Renault','Aston-Martin')").partitionBy("brand","model","year","month").save("/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze")

//checking data

DeltaTable.forPath(spark,"/user/raptor/testing/hadoop/deltaTableTestFolder/outputFiles/carsDeltaTable_Bronze").toDF.filter("brand not in ('Ferrari','Volswogen','Renault','Aston-Martin')")



pipeline-->
