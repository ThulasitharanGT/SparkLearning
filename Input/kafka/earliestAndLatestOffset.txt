push messages to a topic in kafka and then read it from starting and earliest


creating a topic

spark-submit --class org.util.kafka.createTopic  --num-executors 1 --executor-cores 2 --executor-memory 1g --driver-memory 512m --driver-cores 1 --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.12:2.3.1,org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.4 /home/raptor/IdeaProjects/SparkLearning/build/libs/SparkLearning-1.0-SNAPSHOT.jar  topicName=msgCheck topicPartitions=10 topicReplicationFactor=3 zookeeperIp=localhost:3039 isSecure=false sessionTimeOutMilliSecs=200000 connectionTimeOutMilliSecs=15000 maxInFlightRequests=20 metricGroup=myGroup metricType=myType

spark-submit --class com.test.AtomicityUsingMysql.randomMsgProducer --driver-memory 512m --executor-memory 1g --num-executors 1 --executor-cores 2 --driver-cores 1 --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.12:2.3.1 /home/raptor/IdeaProjects/KafkaGradleTest/build/libs/KafkaGradleTest-1.0-SNAPSHOT-all.jar key=check messageLength=5 numOfRecords=50 keySerializer=org.apache.kafka.common.serialization.StringSerializer valueSerializer=org.apache.kafka.common.serialization.StringSerializer bootStrapServer=localhost:9091,localhost:9092,localhost:9093 topicName=msgCheck


spark-shell --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.11:2.3.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 

import sys.process._
"hdfs dfs -rm -r /user/raptor/streams"!

val selectExprSeq=Seq("cast (key as string) key","cast (value as string) value","topic","partition","offset","timestamp","timestampType")

val readstreamDF=spark.readStream.format("kafka").option("checkpointLocation","/user/raptor/streams/checkpoint/temp").option("kafka.bootstrap.servers","localhost:9091,localhost:9092,localhost:9093").option("key.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("value.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("subscribe","msgCheck").option("startingOffsets","earliest").load.selectExpr(selectExprSeq:_*)

val writestreamDF=readstreamDF.writeStream.format("console").outputMode("append").option("checkpointLocation","/user/raptor/streams/checkpoint/temp2")
writestreamDF.start

spark-shell --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.11:2.3.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 

val selectExprSeq=Seq("cast (key as string) key","cast (value as string) value","topic","partition","offset","timestamp","timestampType")

val readstreamDFLatest=spark.readStream.format("kafka").option("checkpointLocation","/user/raptor/streams/checkpoint/temp3").option("kafka.bootstrap.servers","localhost:9091,localhost:9092,localhost:9093").option("key.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("value.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("subscribe","msgCheck").option("startingOffsets","latest").load.selectExpr(selectExprSeq:_*)

val writestreamDFLatest=readstreamDFLatest.writeStream.format("console").outputMode("append").option("checkpointLocation","/user/raptor/streams/checkpoint/temp4")
writestreamDFLatest.start
