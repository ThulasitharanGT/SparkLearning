spark-submit --class org.util.kafka.createTopic  --num-executors 1 --executor-cores 2 --executor-memory 1g --driver-memory 512m --driver-cores 1 --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.12:2.3.1,org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.4 /home/raptor/IdeaProjects/SparkLearning/build/libs/SparkLearning-1.0-SNAPSHOT.jar  topicName=msgCheck topicPartitions=10 topicReplicationFactor=3 zookeeperIp=localhost:3039 isSecure=false sessionTimeOutMilliSecs=200000 connectionTimeOutMilliSecs=15000 maxInFlightRequests=20 metricGroup=myGroup metricType=myType

spark-submit --class com.test.AtomicityUsingMysql.randomMsgProducer --driver-memory 512m --executor-memory 1g --num-executors 1 --executor-cores 2 --driver-cores 1 --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.12:2.3.1 /home/raptor/IdeaProjects/KafkaGradleTest/build/libs/KafkaGradleTest-1.0-SNAPSHOT-all.jar key=check messageLength=5 numOfRecords=50 keySerializer=org.apache.kafka.common.serialization.StringSerializer valueSerializer=org.apache.kafka.common.serialization.StringSerializer bootStrapServer=localhost:9091,localhost:9092,localhost:9093 topicName=msgCheck

spark-submit --class com.test.AtomicityUsingMysql.randomMsgProducer --driver-memory 512m --executor-memory 1g --num-executors 1 --executor-cores 2 --driver-cores 1 --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.12:2.3.1 /home/raptor/IdeaProjects/KafkaGradleTest/build/libs/KafkaGradleTest-1.0-SNAPSHOT-all.jar key=check1 messageLength=5 numOfRecords=50 keySerializer=org.apache.kafka.common.serialization.StringSerializer valueSerializer=org.apache.kafka.common.serialization.StringSerializer bootStrapServer=localhost:9091,localhost:9092,localhost:9093 topicName=msgCheck

spark-submit --class com.test.AtomicityUsingMysql.randomMsgProducer --driver-memory 512m --executor-memory 1g --num-executors 1 --executor-cores 2 --driver-cores 1 --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.12:2.3.1 /home/raptor/IdeaProjects/KafkaGradleTest/build/libs/KafkaGradleTest-1.0-SNAPSHOT-all.jar key=check2 messageLength=5 numOfRecords=50 keySerializer=org.apache.kafka.common.serialization.StringSerializer valueSerializer=org.apache.kafka.common.serialization.StringSerializer bootStrapServer=localhost:9091,localhost:9092,localhost:9093 topicName=msgCheck


spark-submit --class org.util.kafka.deleteTopic  --num-executors 1 --executor-cores 2 --executor-memory 1g --driver-memory 512m --driver-cores 1 --packages org.apache.kafka:kafka-clients:2.3.1,org.apache.kafka:kafka_2.12:2.3.1,org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.4 /home/raptor/IdeaProjects/SparkLearning/build/libs/SparkLearning-1.0-SNAPSHOT.jar  topicName=msgCheck topicPartitions=10 topicReplicationFactor=2 zookeeperIp=localhost:3039 isSecure=false sessionTimeOutMilliSecs=200000 connectionTimeOutMilliSecs=15000 maxInFlightRequests=20 metricGroup=myGroup metricType=myType sleepMsForKafkaToDeleteTopic=60000

spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5

//reading all partitions

import sys.process._
"hdfs dfs -rm -r /user/raptor/streams"!

val selectExprSeq=Seq("cast (key as string) key","cast (value as string) value","topic","partition","offset","timestamp","timestampType")

val readstreamDF=spark.readStream.format("kafka").option("checkpointLocation","/user/raptor/streams/checkpoint/temp").option("kafka.bootstrap.servers","localhost:9091,localhost:9092,localhost:9093").option("key.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("value.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("subscribe","msgCheck").option("startingOffsets","earliest").load.selectExpr(selectExprSeq:_*) //.option("","")

val writestreamDF=readstreamDF.writeStream.format("console").outputMode("append").option("checkpointLocation","/user/raptor/streams/checkpoint/temp2")

writestreamDF.start.awaitTermination(100)

writestreamDF.stop

//"{"topicA":[0,1],"topicB":[2,4]}"

//"""{"msgCheck":{"0":0,"1":1}"""
// -1 latest -2 earliest // any other number from 0 that is the offset number

//specific offsets of all partitions of topic

val readstreamDF=spark.readStream.format("kafka").option("checkpointLocation","/user/raptor/streams/checkpoint/temp").option("kafka.bootstrap.servers","localhost:9091,localhost:9092,localhost:9093").option("key.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("value.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("subscribe","msgCheck").option("startingOffsets","""{"msgCheck":{"0":-1,"1":-2,"2":-2,"3":-2,"4":-2,"5":-2,"6":-2,"7":-2,"8":-2,"9":-2}}""").load.selectExpr(selectExprSeq:_*)

val writestreamDF=readstreamDF.writeStream.format("console").outputMode("append").option("checkpointLocation","/user/raptor/streams/checkpoint/temp2")

writestreamDF.start.awaitTermination(100)

//specific partitions of topic  -- use assigh instead of suscribe

val readstreamDF=spark.readStream.format("kafka").option("checkpointLocation","/user/raptor/streams/checkpoint/temp").option("kafka.bootstrap.servers","localhost:9091,localhost:9092,localhost:9093").option("key.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("value.deserializer","org.apache.kafka.common.serialization.StringSerializer").option("assign","""{"msgCheck":[0,1]}""").option("startingOffsets","earliest").load.selectExpr(selectExprSeq:_*)

val writestreamDF=readstreamDF.writeStream.format("console").outputMode("append").option("checkpointLocation","/user/raptor/streams/checkpoint/temp2")

writestreamDF.start.awaitTermination(100)


