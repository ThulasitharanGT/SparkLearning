val df=spark.read.option("header","true").option("delimiter","|").option("inferSchema","true").csv("file:///home/raptor/IdeaProjects/SparkLearning/Input/temp/cumulative_sum_eg_input.txt")

df.createOrReplaceTempView("inventory")
spark.sql("select Product_Code,Quantity,Inventory_Date, sum(Quantity) over (partition by Product_Code order by Inventory_Date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) cumSum from inventory   order by Product_Code,Inventory_Date").show(false)

import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

df.withColumn("cumSum",avg("Quantity").over(Window.partitionBy($"Product_Code").orderBy($"Inventory_Date"))).orderBy("Product_Code","Inventory_Date").show

df.withColumn("cumSum",sum("Quantity").over(Window.partitionBy($"Product_Code").orderBy($"Inventory_Date"))).orderBy("Product_Code","Inventory_Date").show

//custom way:
============
case class inventoryCumSum (productCode:String,quantity:Int,inventoryDate:String,cumSum:Int)
import scala.collection.mutable.ListBuffer
val resultListBuffer:ListBuffer[inventoryCumSum]= ListBuffer[inventoryCumSum]()
//val distinctProductCode=(df.select("Product_Code").distinct.collect).map(x => x.toString.substring(1,x.toString.length-1)) //.collect reurns array of row, so take the value and index out of row using (index)
val distinctProductCode=(df.select("Product_Code").distinct.collect).map(_(0))
for (productCode <- distinctProductCode)
{
val currentProductCodeDF=df.filter(s"Product_Code='${productCode}'").orderBy("Inventory_Date").collect
var cumSumVar=0
for(currentProductCodeDFRow <- currentProductCodeDF)
{
cumSumVar=cumSumVar+currentProductCodeDFRow(1).toString.toInt
resultListBuffer+=inventoryCumSum(currentProductCodeDFRow(0).toString,currentProductCodeDFRow(1).toString.toInt,currentProductCodeDFRow(2).toString,cumSumVar)
}
}

resultListBuffer.toSeq.toDF.orderBy("productCode","inventoryDate").show

comparison
========

resultListBuffer.toSeq.toDF.orderBy("productCode","inventoryDate").intersect(df.withColumn("cumSum",sum("Quantity").over(Window.partitionBy($"Product_Code").orderBy($"Inventory_Date"))).orderBy("Product_Code","Inventory_Date")).count
resultListBuffer.toSeq.toDF.orderBy("productCode","inventoryDate").count
df.withColumn("cumSum",sum("Quantity").over(Window.partitionBy($"Product_Code").orderBy($"Inventory_Date"))).orderBy("Product_Code","Inventory_Date").count


avg try//
===========
case class inventoryCumAvg (productCode:String,quantity:Int,inventoryDate:String,cumAvg:Double)
import scala.collection.mutable.ListBuffer
val resultListBufferAvg:ListBuffer[inventoryCumAvg]= ListBuffer[inventoryCumAvg]()
//val distinctProductCode=(df.select("Product_Code").distinct.collect).map(x => x.toString.substring(1,x.toString.length-1)) //.collect reurns array of row, so take the value and index out of row using (index)
val distinctProductCode=(df.select("Product_Code").distinct.collect).map(_(0))
for (productCode <- distinctProductCode)
{
val currentProductCodeDF=df.filter(s"Product_Code='${productCode}'").orderBy("Inventory_Date").collect
var cumSumVar:Double=0
var cumAvgVar:Double=0
var productCodeRecordnumber=1
for(currentProductCodeDFRow <- currentProductCodeDF)
{
cumSumVar=cumSumVar+currentProductCodeDFRow(1).toString.toInt
cumAvgVar=cumSumVar/productCodeRecordnumber
resultListBufferAvg+=inventoryCumAvg(currentProductCodeDFRow(0).toString,currentProductCodeDFRow(1).toString.toInt,currentProductCodeDFRow(2).toString,cumAvgVar)
productCodeRecordnumber=productCodeRecordnumber+1
}
}

resultListBufferAvg.toSeq.toDF.orderBy("productCode","inventoryDate").show

df.withColumn("cumSum",avg("Quantity").over(Window.partitionBy($"Product_Code").orderBy($"Inventory_Date"))).orderBy("Product_Code","Inventory_Date").count
resultListBufferAvg.toSeq.toDF.orderBy("productCode","inventoryDate").count
df.withColumn("cumSum",avg("Quantity").over(Window.partitionBy($"Product_Code").orderBy($"Inventory_Date"))).orderBy("Product_Code","Inventory_Date").intersect(resultListBufferAvg.toSeq.toDF.orderBy("productCode","inventoryDate")).count




=========================+++++++++++++++++++++++++++++++++++++============================================++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++================


val transactionDF=spark.read.option("header","true").option("delimiter","|").option("inferSchema","true").csv("file:///home/raptor/IdeaProjects/SparkLearning/Input/temp/rank_input_1.txt")

import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

transactionDF.withColumn("rowNum",rank.over(Window.partitionBy("Account_number").orderBy(desc("transaction_time")))).selectExpr("Account_number","transaction_time","Transaction_id")

transactionDF.withColumn("rowNum",rank.over(Window.partitionBy("Account_number").orderBy(desc("transaction_time")))).filter("rowNum=1").drop("rowNum","transaction_time").orderBy("Account_number").show

//top 5 highest current balance account.
transactionDF.withColumn("rowNum",rank.over(Window.partitionBy("Account_number").orderBy(desc("transaction_time")))).filter("rowNum=1").drop("rowNum","transaction_time").orderBy("Account_number").withColumn("highestBalance",rank.over(Window.orderBy(desc("balance")))).where("highestBalance <=5").drop("highestBalance").show





